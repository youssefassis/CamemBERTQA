{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "FQuAD_with_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1237679854174ede8c50a43bc2959ded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_629f475b5a864bd4aad8a818468f6af6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_77ab2cfc66fc43bd99d64c3b23af5015",
              "IPY_MODEL_346aaf55e312487a99e4b0cde05d9e4f"
            ]
          }
        },
        "629f475b5a864bd4aad8a818468f6af6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77ab2cfc66fc43bd99d64c3b23af5015": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1961201a31674fd09fe3e6c7e5ed0b9a",
            "_dom_classes": [],
            "description": "Iteration: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 307,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 307,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d301f155fe742f380227bd642f7f79e"
          }
        },
        "346aaf55e312487a99e4b0cde05d9e4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6d82aec9181c4eec983619d7eecc41a1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 307/307 [03:47&lt;00:00,  1.35it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_140cac41a7ef43078efbbaeea6b1e8df"
          }
        },
        "1961201a31674fd09fe3e6c7e5ed0b9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d301f155fe742f380227bd642f7f79e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6d82aec9181c4eec983619d7eecc41a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "140cac41a7ef43078efbbaeea6b1e8df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de15e7998e1d4be6990815abef5ae76b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_192681f388e143529f6e0947476881bc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d6b4a1e940584d8baacc871c50430b84",
              "IPY_MODEL_13a35ff8fc544ef7b91c9c56a5748622"
            ]
          }
        },
        "192681f388e143529f6e0947476881bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6b4a1e940584d8baacc871c50430b84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_525adc3eb1a04b2d904e7e204b0a12f9",
            "_dom_classes": [],
            "description": "Iteration: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 307,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 307,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6ef7798abcdd453995297debc25593bb"
          }
        },
        "13a35ff8fc544ef7b91c9c56a5748622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_912c3b45ce27496fb0194346f53f4a4e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 307/307 [03:01&lt;00:00,  1.69it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a42bdfe8862a49b69d78a6375dcf6af3"
          }
        },
        "525adc3eb1a04b2d904e7e204b0a12f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6ef7798abcdd453995297debc25593bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "912c3b45ce27496fb0194346f53f4a4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a42bdfe8862a49b69d78a6375dcf6af3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aafe993eb5df40ad8864e865ee27ec7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a9ff587e125d4845847f70a258098b07",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_86977e5b8e9440cfa82ad86d2b95fc8f",
              "IPY_MODEL_add649e93ac5401da015b083eef29864"
            ]
          }
        },
        "a9ff587e125d4845847f70a258098b07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86977e5b8e9440cfa82ad86d2b95fc8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_aafdfa5290274c7b9e80e00df129c157",
            "_dom_classes": [],
            "description": "Iteration:  55%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 307,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 168,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_213d39e72e6a4168ae82857362f2d0f9"
          }
        },
        "add649e93ac5401da015b083eef29864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2f4badb487ad428093f00896c0d9102a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 168/307 [01:39&lt;01:22,  1.69it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_90cf46ca606c462b96d5125cf0f40ff4"
          }
        },
        "aafdfa5290274c7b9e80e00df129c157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "213d39e72e6a4168ae82857362f2d0f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f4badb487ad428093f00896c0d9102a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "90cf46ca606c462b96d5125cf0f40ff4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6HB-2XIc1wm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from pandas import json_normalize\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33q8KN8Bc1wr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def formatting_squad(dataset_file, display=False):\n",
        "    with open(dataset_file, encoding='utf-8') as f:\n",
        "        # print(json.dumps(f))\n",
        "        raw_data = json.load(f)\n",
        "        raw_data = raw_data['data']\n",
        "        raw_data = json_normalize(raw_data)['paragraphs']\n",
        "        context = []\n",
        "        question = []\n",
        "        answer_start = []\n",
        "        text = []\n",
        "        for i in range(len(raw_data)): # paragraphs\n",
        "            for j in range(len(raw_data[i])): #qas\n",
        "                for k in range(len(raw_data[i][j]['qas'])):\n",
        "                    if (len(raw_data[i][j]['qas'][k]['answers']) != 0):\n",
        "                        question.append(raw_data[i][j]['qas'][k]['question'])\n",
        "                        answer_start.append(raw_data[i][j]['qas'][k]['answers'][0]['answer_start'])\n",
        "                        text.append(raw_data[i][j]['qas'][k]['answers'][0]['text'])\n",
        "                        context.append(raw_data[i][j]['context'])\n",
        "                    else:\n",
        "                        continue\n",
        "        data = pd.DataFrame({\"context\":context, \"question\": question, \"answer_start\": answer_start, \"text\": text})\n",
        "#         print(data.head())\n",
        "        if display is True:\n",
        "            print(data.shape)\n",
        "        return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwSrabJcc1ww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = formatting_squad(r\"train.json\")\n",
        "train_data = train_data[train_data['context'].apply(lambda x: len(x)<500)].reset_index(drop=True)\n",
        "# print(train_data.shape)\n",
        "# train_data.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2MSzbodc1w2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_data = formatting_squad(r\"valid.json\")\n",
        "# valid_data = valid_data[valid_data['context'].apply(lambda x: len(x)<500)].reset_index(drop=True)\n",
        "valid_data = valid_data.reset_index(drop=True)\n",
        "# print(valid_data.shape)\n",
        "# valid_data.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV7ibM74c1w5",
        "colab_type": "text"
      },
      "source": [
        "--------------------------------------------------------------------------------------------------------\n",
        "--------------------------------------------------------------------------------------------------------\n",
        "--------------------------------------------------------------------------------------------------------\n",
        "--------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vWRTSezexw4",
        "colab_type": "code",
        "outputId": "cd196d15-7321-43d9-f80f-26a1580c1e51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.9.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.90)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTzRl1Rxc1w6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import transformers\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers.optimization import AdamW\n",
        "from tqdm import trange, tqdm_notebook\n",
        "# import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lW12FVlc1xA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers.modeling_camembert import CamembertModel\n",
        "from transformers.tokenization_camembert import CamembertTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMcgRdvKDva_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import math\n",
        "# # import torch.nn as nn\n",
        "\n",
        "# class PositionalEncoding(nn.Module):\n",
        "#     def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "#         super(PositionalEncoding, self).__init__()\n",
        "#         self.dropout = nn.Dropout(p=dropout)\n",
        "#         self.d_model = d_model\n",
        "\n",
        "#         pe = torch.zeros(max_len, d_model)\n",
        "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
        "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
        "#         pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "#         self.register_buffer('pe', pe)\n",
        "\n",
        "#     def forward(self, x, seq_len = 768, mask = None):\n",
        "#         pos_emb = self.pe[:, :seq_len]\n",
        "#         x = x * mask[:, :, None].float()\n",
        "#         x = x + pos_emb\n",
        "#         return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs62dJHXc1xH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class CamemBERTQA(nn.Module):\n",
        "#     def __init__(self,bert_type, hidden_size, num_labels, num_inter_layers=1, heads = 12, do_lower_case = True):\n",
        "#         super(CamemBERTQA, self).__init__()\n",
        "#         self.do_lower_case = do_lower_case\n",
        "#         self.bert_type = bert_type\n",
        "#         self.hidden_size = hidden_size\n",
        "#         self.num_labels = num_labels       \n",
        "#         self.num_inter_layers = num_inter_layers\n",
        "#         self.camembert = CamembertModel.from_pretrained(self.bert_type)\n",
        "\n",
        "#         # ---------------------------------------------------------------------------------------------------------------------\n",
        "#         self.d_model = self.hidden_size # 768\n",
        "#         dim_feedforward = self.hidden_size * 4  # 2048\n",
        "#         dropout = 0.1\n",
        "#         self.pos_emb = PositionalEncoding(d_model = self.d_model, dropout = dropout)\n",
        "#         self.transformer_inter = nn.ModuleList(\n",
        "#             [nn.TransformerEncoderLayer(d_model = self.d_model, nhead = heads, dim_feedforward = 2048, dropout = dropout)  #dim_feedforward=2048, dropout=0.1)\n",
        "#              for _ in range(num_inter_layers)])\n",
        "#         # ---------------------------------------------------------------------------------------------------------------------\n",
        "#         self.qa_outputs = nn.Linear(self.hidden_size, self.num_labels)\n",
        "\n",
        "#     def forward(self, input_ids, mask=None):\n",
        "#         bert_output = self.camembert(input_ids = input_ids) # input_ids is a tensor\n",
        "        \n",
        "#         # ---------------------------------------------------------------------------------------------------------------------\n",
        "#         seq_len = self.hidden_size\n",
        "#         x = self.pos_emb(x = bert_output, seq_len = seq_len, mask = None)\n",
        "\n",
        "#         for i in range(self.num_inter_layers):\n",
        "#             x = self.transformer_inter[i](i, x, x, 1 - mask)  # all_tokens * max_tokens * dim\n",
        "#         output = self.layer_norm(x) # Transformer also normalizes the outputs from each layer.\n",
        "#         # ---------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "#         sequence_output = output[0]   #(None, seq_len, hidden_size)\n",
        "#         logits = self.qa_outputs(sequence_output) #(None, seq_len, hidden_size)*(hidden_size, 2)=(None, seq_len, 2)\n",
        "#         start_logits, end_logits = logits.split(1, dim=-1)    #(None, seq_len, 1), (None, seq_len, 1)\n",
        "#         start_logits = start_logits.squeeze(-1)  #(None, seq_len)\n",
        "#         end_logits = end_logits.squeeze(-1)    #(None, seq_len)\n",
        "#         outputs = (start_logits, end_logits,) \n",
        "#         return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TPtJ7kILLDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import logging\n",
        "# logger = logging.getLogger(\"tensor_shapes\")\n",
        "# handler = logging.StreamHandler()\n",
        "# formatter = logging.Formatter(\n",
        "#         '%(message)s')\n",
        "# handler.setFormatter(formatter)\n",
        "# logger.addHandler(handler)\n",
        "# # if you want the model to continuously print tensor shapes, set to DEBUG!\n",
        "# logger.setLevel(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ8YETBxLOa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import inspect\n",
        "# def getclass():\n",
        "#     stack = inspect.stack()\n",
        "#     return stack[3][0].f_locals[\"self\"].__class__\n",
        "\n",
        "# # A helper function to check how tensor sizes change\n",
        "# def log_size(tsr: torch.Tensor, name: str):\n",
        "#     cls = getclass()\n",
        "#     logger.log(level=cls.level, msg=f\"[{cls.__name__}] {name} size={tsr.shape}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEEpsAFIMNVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from enum import IntEnum\n",
        "# # Control how much debugging output we want\n",
        "# class TensorLoggingLevels(IntEnum):\n",
        "#     attention = 1\n",
        "#     attention_head = 2\n",
        "#     multihead_attention_block = 3\n",
        "#     enc_dec_block = 4\n",
        "#     enc_dec = 5\n",
        "class Dim(IntEnum):\n",
        "    batch = 0\n",
        "    seq = 1\n",
        "    feature = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3DBfDvECrUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    # level = TensorLoggingLevels.attention # Logging level: \n",
        "    def __init__(self, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        d_k = k.size(-1) # get the size of the key\n",
        "        assert q.size(-1) == d_k\n",
        "\n",
        "        # compute the dot product between queries and keys for\n",
        "        # each batch and position in the sequence\n",
        "        attn = torch.bmm(q, k.transpose(Dim.seq, Dim.feature)) # (Batch, Seq, Seq)\n",
        "        # we get an attention score between each position in the sequence\n",
        "        # for each batch\n",
        "\n",
        "        # scale the dot products by the dimensionality (see the paper for why we do this!)\n",
        "        attn = attn / math.sqrt(d_k)\n",
        "        # normalize the weights across the sequence dimension\n",
        "        # (Note that since we transposed, the sequence and feature dimensions are switched)\n",
        "        attn = torch.exp(attn)\n",
        "        # log_size(attn, \"attention weight\") # (Batch, Seq, Seq)\n",
        "        \n",
        "        # fill attention weights with 0s where padded\n",
        "        if mask is not None: attn = attn.masked_fill(mask, 0)\n",
        "        attn = attn / attn.sum(dim=-1, keepdim=True)\n",
        "        attn = self.dropout(attn)\n",
        "        output = torch.bmm(attn, v) # (Batch, Seq, Feature)\n",
        "        # log_size(output, \"attention output size\") # (Batch, Seq, Seq)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjQ2-jTuC7iS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    # level = TensorLoggingLevels.attention_head\n",
        "    def __init__(self, d_model, d_feature, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # We will assume the queries, keys, and values all have the same feature size\n",
        "        self.attn = ScaledDotProductAttention(dropout)\n",
        "        self.query_tfm = nn.Linear(d_model, d_feature)\n",
        "        self.key_tfm = nn.Linear(d_model, d_feature)\n",
        "        self.value_tfm = nn.Linear(d_model, d_feature)\n",
        "\n",
        "    def forward(self, queries, keys, values, mask=None):\n",
        "        Q = self.query_tfm(queries) # (Batch, Seq, Feature)\n",
        "        K = self.key_tfm(keys) # (Batch, Seq, Feature)\n",
        "        V = self.value_tfm(values) # (Batch, Seq, Feature)\n",
        "        # log_size(Q, \"queries, keys, vals\")\n",
        "        # compute multiple attention weighted sums\n",
        "        x = self.attn(Q, K, V)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_p2ihrbDCup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    # level = TensorLoggingLevels.multihead_attention_block\n",
        "    def __init__(self, d_model, d_feature, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_feature = d_feature\n",
        "        self.n_heads = n_heads\n",
        "        # in practice, d_model == d_feature * n_heads\n",
        "        assert d_model == d_feature * n_heads\n",
        "\n",
        "        # Note that this is very inefficient:\n",
        "        # I am merely implementing the heads separately because it is \n",
        "        # easier to understand this way\n",
        "        self.attn_heads = nn.ModuleList([\n",
        "            AttentionHead(d_model, d_feature, dropout) for _ in range(n_heads)\n",
        "        ])\n",
        "        self.projection = nn.Linear(d_feature * n_heads, d_model) \n",
        "    \n",
        "    def forward(self, queries, keys, values, mask=None):\n",
        "        # log_size(queries, \"Input queries\")\n",
        "        x = [attn(queries, keys, values, mask=mask) # (Batch, Seq, Feature)\n",
        "             for i, attn in enumerate(self.attn_heads)]\n",
        "        # log_size(x[0], \"output of single head\")\n",
        "        \n",
        "        # reconcatenate\n",
        "        x = torch.cat(x, dim=Dim.feature) # (Batch, Seq, D_Feature * n_heads)\n",
        "        # log_size(x, \"concatenated output\")\n",
        "        x = self.projection(x) # (Batch, Seq, D_Model)\n",
        "        # log_size(x, \"projected output\")\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BdTsk8tDNET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-8):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaaRgMv7DQEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    # level = TensorLoggingLevels.enc_dec_block\n",
        "    # def __init__(self, d_model=512, d_feature=64,\n",
        "                #  d_ff=2048, n_heads=8, dropout=0.1):\n",
        "    def __init__(self, d_model=768, d_feature=64,\n",
        "                 d_ff=3072, n_heads=12, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
        "        self.layer_norm1 = LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.position_wise_feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "        )\n",
        "        self.layer_norm2 = LayerNorm(d_model)\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        # log_size(x, \"Encoder block input\")\n",
        "        att = self.attn_head(x, x, x, mask=mask)\n",
        "        # log_size(x, \"Attention output\")\n",
        "        # Apply normalization and residual connection\n",
        "        x = x + self.dropout(self.layer_norm1(att))\n",
        "        # Apply position-wise feedforward network\n",
        "        pos = self.position_wise_feed_forward(x)\n",
        "        # log_size(x, \"Feedforward output\")\n",
        "        # Apply normalization and residual connection\n",
        "        x = x + self.dropout(self.layer_norm2(pos))\n",
        "        # log_size(x, \"Encoder size output\")\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaI-o90kDXA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    # level = TensorLoggingLevels.enc_dec\n",
        "    # def __init__(self, n_blocks=6, d_model=512,\n",
        "                #  n_heads=8, d_ff=2048, dropout=0.1):\n",
        "    def __init__(self, n_blocks=2, d_model=768 # 768\n",
        "                 , n_heads=12, d_ff=3072, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoders = nn.ModuleList([\n",
        "            EncoderBlock(d_model=d_model, d_feature=d_model // n_heads,\n",
        "                         d_ff=d_ff, dropout=dropout)\n",
        "            for _ in range(n_blocks)\n",
        "        ])\n",
        "    \n",
        "    def forward(self, x: torch.FloatTensor, mask=None):\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHs5GAj4DcTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    # level = 1\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.weight = nn.Parameter(pe, requires_grad=False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.weight[:, :x.size(1), :] # (1, Seq, Feature)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjm0tmGdF69K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CamemBERTQA(nn.Module):\n",
        "    def __init__(self,bert_type, hidden_size, num_labels, n_blocks = 2, n_heads = 12, dropout=0.1):\n",
        "        super(CamemBERTQA, self).__init__()\n",
        "        self.bert_type = bert_type\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_labels = num_labels\n",
        "        self.n_blocks = n_blocks\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout = dropout\n",
        "        self.camembert = CamembertModel.from_pretrained(self.bert_type)\n",
        "        self.transformer = TransformerEncoder(n_blocks = self.n_blocks, d_model = self.hidden_size\n",
        "                                              , n_heads=self.n_heads, d_ff=3072, dropout=self.dropout)\n",
        "        self.qa_outputs = nn.Linear(self.hidden_size, self.num_labels)\n",
        "\n",
        "    def forward(self, input_ids, mask=None):\n",
        "        output = self.camembert(input_ids = input_ids) # input_ids is a tensor\n",
        "        sequence_output = self.transformer(output[0]) # Transformer\n",
        "        logits = self.qa_outputs(sequence_output) #(None, seq_len, hidden_size)*(hidden_size, 2)=(None, seq_len, 2)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)    #(None, seq_len, 1), (None, seq_len, 1)\n",
        "        start_logits = start_logits.squeeze(-1)  #(None, seq_len)\n",
        "        end_logits = end_logits.squeeze(-1)    #(None, seq_len)\n",
        "        outputs = (start_logits, end_logits,) \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGMLNjxdc1xN",
        "colab_type": "text"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA0aIxDFc1xP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_func(out, s_target, e_target):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    s_loss = criterion(out[0], s_target)\n",
        "    e_loss = criterion(out[1], e_target)\n",
        "    total_loss = s_loss+e_loss\n",
        "    return total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIJmNaXVc1zD",
        "colab_type": "text"
      },
      "source": [
        "# Dataloader for train and eval set:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lkZAMd_c1zE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertDatasetModule(Dataset):\n",
        "    def __init__(self, tokenizer, context, question, max_length, text):\n",
        "        self.context = context\n",
        "        self.question = question\n",
        "        self.text = text\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.context)\n",
        "  \n",
        "    def __getitem__(self, idx):\n",
        "        context_ = self.context[idx]\n",
        "        question_ = self.question[idx]\n",
        "        text_ = self.text[idx]\n",
        "    \n",
        "        #encoding\n",
        "        input_ids = self.tokenizer.encode(question_, context_)\n",
        "        answer_ids = self.tokenizer.encode(text_)\n",
        "        token_type_ids = [0 if i <= input_ids.index(6) else 1 for i in range(len(input_ids))]\n",
        "    \n",
        "        #calculating start and end position of the answer in input_ids\n",
        "        s_pos, e_pos = 0, 0\n",
        "        for i in range(len(input_ids)):\n",
        "            if (input_ids[i: i+len(answer_ids[1:-1])] == answer_ids[1:-1]):\n",
        "                s_pos = i\n",
        "                e_pos = i + len(answer_ids[1:-1]) - 1\n",
        "                break\n",
        "\n",
        "        assert((s_pos<len(input_ids)) & (e_pos<len(input_ids)) & (s_pos<=e_pos))\n",
        "\n",
        "        if (len(input_ids) < self.max_length):\n",
        "            padding_len = self.max_length - len(input_ids)\n",
        "            ids = input_ids + ([0]*padding_len)\n",
        "        else:\n",
        "            ids = input_ids[:self.max_length]\n",
        "\n",
        "        if (len(token_type_ids)<self.max_length):\n",
        "            padding_len = self.max_length - len(token_type_ids)\n",
        "            token_ids = token_type_ids  + ([1]*padding_len)\n",
        "        else:\n",
        "            token_ids = token_type_ids[:self.max_length]\n",
        "\n",
        "        return {'ids': torch.tensor(ids, dtype = torch.long),\n",
        "                'token_type_ids': torch.tensor(token_ids, dtype = torch.long),\n",
        "                'start_pos': torch.tensor(s_pos, dtype = torch.long),\n",
        "                'end_pos': torch.tensor(e_pos, dtype = torch.long)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGMMSzQQc1zc",
        "colab_type": "text"
      },
      "source": [
        "# Training and evaluation function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WATYoDQc1zd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_loop(dataloader, model, optimizer, device, max_grad_norm, scheduler=None):\n",
        "    model.train()\n",
        "    for bi, d in enumerate(tqdm_notebook(dataloader, desc=\"Iteration\")):\n",
        "        ids = d['ids']\n",
        "        # mask_ids = d['mask']\n",
        "        token_ids = d['token_type_ids']\n",
        "        start_pos = d['start_pos']\n",
        "        end_pos = d['end_pos']\n",
        "\n",
        "        ids = ids.to(device, dtype = torch.long)\n",
        "        # mask_ids = mask_ids.to(device, dtype = torch.long)\n",
        "        token_ids = token_ids.to(device, dtype = torch.long)\n",
        "        start_pos = start_pos.to(device, dtype = torch.long)\n",
        "        end_pos = end_pos.to(device, dtype = torch.long)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        start_and_end_scores = model(ids)\n",
        "        # start_scores, end_scores = model(ids, token_ids)\n",
        "        loss = loss_func(start_and_end_scores, start_pos, end_pos)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        if bi%100==0:\n",
        "            print (f\"bi: {bi}, loss: {loss}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxXOQfKjc1zg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_loop(dataloader, model, device):\n",
        "    model.eval()\n",
        "    pred_s = None\n",
        "    pred_e = None\n",
        "    eval_loss = 0.0\n",
        "    eval_steps = 0\n",
        "\n",
        "    for bi, d in enumerate(dataloader):\n",
        "        ids = d['ids']\n",
        "        # mask_ids = d['mask']\n",
        "        token_ids = d['token_type_ids']\n",
        "        start_pos = d['start_pos']\n",
        "        end_pos = d['end_pos']\n",
        "\n",
        "        ids = ids.to(device, dtype = torch.long)\n",
        "        # mask_ids = mask_ids.to(device, dtype = torch.long)\n",
        "        token_ids = token_ids.to(device, dtype = torch.long)\n",
        "        start_pos = start_pos.to(device, dtype = torch.long)\n",
        "        end_pos = end_pos.to(device, dtype = torch.long)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            start_and_end_scores = model(ids)\n",
        "#             start_and_end_scores = model(ids, token_ids)\n",
        "\n",
        "            loss = loss_func(start_and_end_scores, start_pos, end_pos)\n",
        "            eval_loss += loss.mean().item()\n",
        "\n",
        "        eval_steps += 1\n",
        "        if pred_s is None:\n",
        "            pred_s = start_and_end_scores[0].detach().cpu().numpy()\n",
        "            pred_e = start_and_end_scores[1].detach().cpu().numpy()\n",
        "        else:\n",
        "            pred_s = np.append(pred_s, start_and_end_scores[0].detach().cpu().numpy(), axis=0)\n",
        "            pred_e = np.append(pred_e, start_and_end_scores[1].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "    eval_loss = eval_loss/eval_steps\n",
        "    pred_start = np.argmax(pred_s, axis=1)\n",
        "    pred_end = np.argmax(pred_e, axis=1)\n",
        "\n",
        "    return eval_loss, pred_start, pred_end"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45tkjXa6c1zj",
        "colab_type": "text"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbcuyRt0c1zk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQ_LENGTH = 512\n",
        "TRAIN_BATCH_SIZE = 16  # 16, 32  it was 8\n",
        "EVAL_BATCH_SIZE = 16 # 8 WAS\n",
        "LEARNING_RATE = 1e-5 #1e-5 or 5e-5\n",
        "NUM_TRAIN_EPOCHS = 3 # or 2, 3, -> 4\n",
        "BERT_TYPE = \"camembert-base\"#\"fmikaelian/camembert-base-fquad\"\n",
        "max_grad_norm = 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hez00STc1zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = CamembertTokenizer.from_pretrained(BERT_TYPE, do_lower_case = True)\n",
        "\n",
        "train_dataset = BertDatasetModule(\n",
        "    tokenizer = tokenizer,\n",
        "    context = train_data['context'],\n",
        "    question = train_data['question'],\n",
        "    max_length = MAX_SEQ_LENGTH,\n",
        "    text = train_data['text']\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = TRAIN_BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eTuPkh_c1zt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_dataset = BertDatasetModule(\n",
        "    tokenizer = tokenizer,\n",
        "    context = valid_data['context'],\n",
        "    question = valid_data['question'],\n",
        "    max_length = MAX_SEQ_LENGTH,\n",
        "    text = valid_data['text']\n",
        ")\n",
        "\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size = EVAL_BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRn0xkjHc1zx",
        "colab_type": "code",
        "outputId": "7d62cce9-a68d-4273-8d45-b192f2c67664",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# import transformers\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "model = CamemBERTQA(bert_type = BERT_TYPE, hidden_size = 768, num_labels = 2, n_blocks = 2, n_heads = 12).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, correct_bias=False)\n",
        "\n",
        "NUM_TRAIN_STEPS = int(len(train_dataset)/TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS) \n",
        "scheduler = transformers.get_constant_schedule_with_warmup(\n",
        "                optimizer,\n",
        "                num_warmup_steps=500,\n",
        "#                 num_training_steps=NUM_TRAIN_STEPS,\n",
        "                last_epoch=-1)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7-0-fiKc1z0",
        "colab_type": "code",
        "outputId": "b1d5e899-2d64-4ef8-b55e-16630c06946d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "NUM_TRAIN_STEPS"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1225"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9SFWsFHn43o",
        "colab_type": "code",
        "outputId": "4ca64fdb-d6eb-45b4-e033-8189328ee9ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.eval()\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CamemBERTQA(\n",
              "  (camembert): CamembertModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (transformer): TransformerEncoder(\n",
              "    (encoders): ModuleList(\n",
              "      (0): EncoderBlock(\n",
              "        (attn_head): MultiHeadAttention(\n",
              "          (attn_heads): ModuleList(\n",
              "            (0): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (1): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (2): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (3): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (4): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (5): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (6): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (7): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (8): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (9): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (10): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (11): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (layer_norm1): LayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (position_wise_feed_forward): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (layer_norm2): LayerNorm()\n",
              "      )\n",
              "      (1): EncoderBlock(\n",
              "        (attn_head): MultiHeadAttention(\n",
              "          (attn_heads): ModuleList(\n",
              "            (0): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (1): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (2): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (3): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (4): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (5): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (6): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (7): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (8): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (9): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (10): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "            (11): AttentionHead(\n",
              "              (attn): ScaledDotProductAttention(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (query_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (key_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value_tfm): Linear(in_features=768, out_features=64, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (layer_norm1): LayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (position_wise_feed_forward): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (layer_norm2): LayerNorm()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1JqNQ9KSrvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Get all of the model's parameters as a list of tuples.\n",
        "# params = list(model.named_parameters())\n",
        "\n",
        "# print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "# print('==== Embedding Layer ====\\n')\n",
        "\n",
        "# for p in params[0:5]:\n",
        "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "# print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "# for p in params[5:21]:\n",
        "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "# print('\\n==== Encoder Layer ====\\n')\n",
        "\n",
        "# for p in params[21:]:\n",
        "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VlWfnv_c1z4",
        "colab_type": "text"
      },
      "source": [
        "# Training Iterations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP3HtdA_c1z5",
        "colab_type": "code",
        "outputId": "56b95339-8677-4593-9075-837ceb9459d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400,
          "referenced_widgets": [
            "1237679854174ede8c50a43bc2959ded",
            "629f475b5a864bd4aad8a818468f6af6",
            "77ab2cfc66fc43bd99d64c3b23af5015",
            "346aaf55e312487a99e4b0cde05d9e4f",
            "1961201a31674fd09fe3e6c7e5ed0b9a",
            "6d301f155fe742f380227bd642f7f79e",
            "6d82aec9181c4eec983619d7eecc41a1",
            "140cac41a7ef43078efbbaeea6b1e8df",
            "de15e7998e1d4be6990815abef5ae76b",
            "192681f388e143529f6e0947476881bc",
            "d6b4a1e940584d8baacc871c50430b84",
            "13a35ff8fc544ef7b91c9c56a5748622",
            "525adc3eb1a04b2d904e7e204b0a12f9",
            "6ef7798abcdd453995297debc25593bb",
            "912c3b45ce27496fb0194346f53f4a4e",
            "a42bdfe8862a49b69d78a6375dcf6af3",
            "aafe993eb5df40ad8864e865ee27ec7a",
            "a9ff587e125d4845847f70a258098b07",
            "86977e5b8e9440cfa82ad86d2b95fc8f",
            "add649e93ac5401da015b083eef29864",
            "aafdfa5290274c7b9e80e00df129c157",
            "213d39e72e6a4168ae82857362f2d0f9",
            "2f4badb487ad428093f00896c0d9102a",
            "90cf46ca606c462b96d5125cf0f40ff4"
          ]
        }
      },
      "source": [
        "#training\n",
        "for epoch in trange(NUM_TRAIN_EPOCHS):\n",
        "    train_loop(train_dataloader, model, optimizer, device, max_grad_norm, scheduler)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1237679854174ede8c50a43bc2959ded",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=307.0, style=ProgressStyle(description_wi…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "bi: 0, loss: 12.445550918579102\n",
            "bi: 100, loss: 9.353384017944336\n",
            "bi: 200, loss: 8.478275299072266\n",
            "bi: 300, loss: 6.773229122161865\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 1/3 [03:01<06:02, 181.39s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de15e7998e1d4be6990815abef5ae76b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=307.0, style=ProgressStyle(description_wi…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "bi: 0, loss: 10.100625038146973\n",
            "bi: 100, loss: 10.137435913085938\n",
            "bi: 200, loss: 8.533740997314453\n",
            "bi: 300, loss: 8.445728302001953\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 2/3 [06:02<03:01, 181.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aafe993eb5df40ad8864e865ee27ec7a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=307.0, style=ProgressStyle(description_wi…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "bi: 0, loss: 6.583135604858398\n",
            "bi: 100, loss: 7.353538990020752\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRuMeshwc1z-",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYM-zXjXc1z_",
        "colab_type": "code",
        "outputId": "9e70e426-259e-4d23-d0a6-27f062c50fad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "res = eval_loop(eval_dataloader, model, device)\n",
        "print(res[0])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (15 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (19 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (16 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (15 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (12 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (18 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (17 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (15 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (8 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (16 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (12 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (8 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (7 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (12 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (19 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (8 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (8 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (15 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (18 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (20 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (12 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (19 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (15 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (27 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (12 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (28 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (21 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (23 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (28 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (21 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (18 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (23 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (19 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (19 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (25 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (30 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (8 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "12.427672094569768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j1-edsKTjUT",
        "colab_type": "text"
      },
      "source": [
        "## Test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N0o9lOWBP68",
        "colab_type": "code",
        "outputId": "6d11fa01-a83e-4f35-b07c-3ea1fab50767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "# path_ali_2 = r\"SQuAD-v1.1-dev.json\"\n",
        "path_ali_2= r\"valid.json\"\n",
        "dev_data = formatting_squad(path_ali_2)\n",
        "# train_data = train_data[train_data['context'].apply(lambda x: len(x)<500)].reset_index(drop=True)\n",
        "print(dev_data.shape)\n",
        "dev_data.head(3)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3188, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Les deux tableaux sont certes décrits par des ...</td>\n",
              "      <td>Que concerne principalement les documents ?</td>\n",
              "      <td>161</td>\n",
              "      <td>La Vierge aux rochers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Les deux tableaux sont certes décrits par des ...</td>\n",
              "      <td>Par quoi sont décrit les deux tableaux ?</td>\n",
              "      <td>46</td>\n",
              "      <td>documents contemporains</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Les deux tableaux sont certes décrits par des ...</td>\n",
              "      <td>Quels types d'objets sont les deux tableaux au...</td>\n",
              "      <td>204</td>\n",
              "      <td>objets de spéculations</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             context  ...                     text\n",
              "0  Les deux tableaux sont certes décrits par des ...  ...    La Vierge aux rochers\n",
              "1  Les deux tableaux sont certes décrits par des ...  ...  documents contemporains\n",
              "2  Les deux tableaux sont certes décrits par des ...  ...   objets de spéculations\n",
              "\n",
              "[3 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ09nFKHc10F",
        "colab_type": "code",
        "outputId": "aa569719-f316-4402-bc02-8936c915913b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "context_ = dev_data['context']\n",
        "question_ = dev_data['question']\n",
        "text_ = dev_data['text']\n",
        "pred_start = res[1]\n",
        "pred_end = res[2]\n",
        "res_text_ = []\n",
        "act_start = []\n",
        "act_end = []\n",
        "\n",
        "\n",
        "input_ids_list = list(map(lambda x,y: tokenizer.encode(x, y), question_, context_))\n",
        "answer_ids_list = list(map(lambda x: tokenizer.encode(x), text_))\n",
        "\n",
        "for i in range(len(input_ids_list)):\n",
        "    res_text_.append(tokenizer.decode(input_ids_list[i][pred_start[i]:pred_end[i]+1]))\n",
        "    s_pos, e_pos = 0, 0\n",
        "    for j in range(len(input_ids_list[i])):\n",
        "        if (input_ids_list[i][j: j+len(answer_ids_list[i][1:-1])] == answer_ids_list[i][1:-1]):\n",
        "            s_pos = j\n",
        "            e_pos = j + len(answer_ids_list[i][1:-1]) - 1\n",
        "            break\n",
        "    act_start.append(s_pos)\n",
        "    act_end.append(e_pos)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (15 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (19 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (16 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (15 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (12 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (18 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (17 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (15 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (8 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (16 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (12 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (8 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (7 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (12 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (19 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (8 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (8 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (15 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (18 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (20 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (12 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (19 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (15 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (27 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (12 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (28 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (21 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (23 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (28 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (21 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (18 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (23 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (19 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (19 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (25 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (30 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (8 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyCvqLvAc10J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_data['start_pos'] = act_start\n",
        "dev_data['end_pos'] = act_end\n",
        "dev_data['predicted_text'] = res_text_\n",
        "dev_data['predicted_start_pos'] = pred_start\n",
        "dev_data['predicted_end_pos'] = pred_end"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kSPV4IBc10O",
        "colab_type": "code",
        "outputId": "80ba149d-b9c9-4832-ef41-d87136e67aef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        }
      },
      "source": [
        "show_columns = ['text', 'predicted_text', 'start_pos', 'end_pos', 'predicted_start_pos', 'predicted_end_pos']\n",
        "dev_data[show_columns].head(20)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>predicted_text</th>\n",
              "      <th>start_pos</th>\n",
              "      <th>end_pos</th>\n",
              "      <th>predicted_start_pos</th>\n",
              "      <th>predicted_end_pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>La Vierge aux rochers</td>\n",
              "      <td>la vierge aux rochers</td>\n",
              "      <td>39</td>\n",
              "      <td>42</td>\n",
              "      <td>39</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>documents contemporains</td>\n",
              "      <td>documents contemporains à leur création</td>\n",
              "      <td>20</td>\n",
              "      <td>21</td>\n",
              "      <td>20</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>objets de spéculations</td>\n",
              "      <td>la vierge aux rochers</td>\n",
              "      <td>56</td>\n",
              "      <td>59</td>\n",
              "      <td>47</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>droite</td>\n",
              "      <td>jambe droite</td>\n",
              "      <td>62</td>\n",
              "      <td>62</td>\n",
              "      <td>61</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gauche</td>\n",
              "      <td>leur pied gauche</td>\n",
              "      <td>63</td>\n",
              "      <td>63</td>\n",
              "      <td>61</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>vert</td>\n",
              "      <td>vert</td>\n",
              "      <td>176</td>\n",
              "      <td>176</td>\n",
              "      <td>176</td>\n",
              "      <td>176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>atelier de Léonard de Vinci</td>\n",
              "      <td>l'atelier de léonard de vinci</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>séchage</td>\n",
              "      <td>processus de séchage</td>\n",
              "      <td>126</td>\n",
              "      <td>126</td>\n",
              "      <td>124</td>\n",
              "      <td>126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>doigts</td>\n",
              "      <td>doigts</td>\n",
              "      <td>91</td>\n",
              "      <td>91</td>\n",
              "      <td>91</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>La Vierge aux rochers</td>\n",
              "      <td>&lt;s&gt; quel est le nom du panneau central du reta...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>trois</td>\n",
              "      <td>trois</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Léonard de Vinci</td>\n",
              "      <td>giovanni ambrogio de predis puisque ce dernier...</td>\n",
              "      <td>70</td>\n",
              "      <td>75</td>\n",
              "      <td>31</td>\n",
              "      <td>217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>une Vierge à l'Enfant</td>\n",
              "      <td>une vierge à l'enfant</td>\n",
              "      <td>321</td>\n",
              "      <td>326</td>\n",
              "      <td>321</td>\n",
              "      <td>326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>fin des années 1480</td>\n",
              "      <td>fin des années 1480</td>\n",
              "      <td>373</td>\n",
              "      <td>377</td>\n",
              "      <td>373</td>\n",
              "      <td>377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>description</td>\n",
              "      <td>leur description</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>19</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>vert</td>\n",
              "      <td>vert</td>\n",
              "      <td>51</td>\n",
              "      <td>51</td>\n",
              "      <td>51</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>luth</td>\n",
              "      <td>luth</td>\n",
              "      <td>41</td>\n",
              "      <td>42</td>\n",
              "      <td>41</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1490</td>\n",
              "      <td>1490</td>\n",
              "      <td>40</td>\n",
              "      <td>41</td>\n",
              "      <td>40</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>juillet</td>\n",
              "      <td>juillet</td>\n",
              "      <td>204</td>\n",
              "      <td>204</td>\n",
              "      <td>204</td>\n",
              "      <td>204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Jean l'évangéliste de La Cène</td>\n",
              "      <td>jean l'évangéliste de la cène</td>\n",
              "      <td>115</td>\n",
              "      <td>124</td>\n",
              "      <td>115</td>\n",
              "      <td>124</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             text  ... predicted_end_pos\n",
              "0           La Vierge aux rochers  ...                42\n",
              "1         documents contemporains  ...                24\n",
              "2          objets de spéculations  ...                50\n",
              "3                          droite  ...                62\n",
              "4                          gauche  ...                63\n",
              "5                            vert  ...               176\n",
              "6     atelier de Léonard de Vinci  ...                35\n",
              "7                         séchage  ...               126\n",
              "8                          doigts  ...                91\n",
              "9           La Vierge aux rochers  ...                67\n",
              "10                          trois  ...                50\n",
              "11               Léonard de Vinci  ...               217\n",
              "12          une Vierge à l'Enfant  ...               326\n",
              "13            fin des années 1480  ...               377\n",
              "14                    description  ...                20\n",
              "15                           vert  ...                51\n",
              "16                           luth  ...                42\n",
              "17                           1490  ...                41\n",
              "18                        juillet  ...               204\n",
              "19  Jean l'évangéliste de La Cène  ...               124\n",
              "\n",
              "[20 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idkQ1cFLc10X",
        "colab_type": "code",
        "outputId": "76d3f47e-1999-41cf-d576-2a9208bcb764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cond1 = dev_data['predicted_start_pos']>dev_data['predicted_end_pos']\n",
        "cond2 = dev_data['end_pos']<dev_data['predicted_start_pos']\n",
        "cond3 = dev_data['start_pos']>dev_data['predicted_end_pos']\n",
        "\n",
        "incorrect_pred = dev_data[(cond1) | (cond2) | (cond3)].shape[0]\n",
        "incorrect_pred"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "910"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqYSmIlIc10e",
        "colab_type": "code",
        "outputId": "a4f3346f-370e-4712-cfd4-8ded80050daa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "t = dev_data.shape[0]\n",
        "print(f\"accuracy = {(t - incorrect_pred)*100/t}\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy = 71.45545796737767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l17zPa_nc10k",
        "colab_type": "code",
        "outputId": "cc782b0a-5c9a-4908-b53c-f886aa288c66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "dev_data[(cond1) | (cond2) | (cond3)][show_columns].head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>predicted_text</th>\n",
              "      <th>start_pos</th>\n",
              "      <th>end_pos</th>\n",
              "      <th>predicted_start_pos</th>\n",
              "      <th>predicted_end_pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>objets de spéculations</td>\n",
              "      <td>la vierge aux rochers</td>\n",
              "      <td>56</td>\n",
              "      <td>59</td>\n",
              "      <td>47</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>atelier de Léonard de Vinci</td>\n",
              "      <td>l'atelier de léonard de vinci</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>La Vierge aux rochers</td>\n",
              "      <td>(la vierge aux rochers)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>63</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>adhérence</td>\n",
              "      <td>problèmes d'adhérence</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>33</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>la tête</td>\n",
              "      <td></td>\n",
              "      <td>74</td>\n",
              "      <td>75</td>\n",
              "      <td>74</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           text  ... predicted_end_pos\n",
              "2        objets de spéculations  ...                50\n",
              "6   atelier de Léonard de Vinci  ...                35\n",
              "9         La Vierge aux rochers  ...                68\n",
              "40                    adhérence  ...                36\n",
              "44                      la tête  ...                45\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2ny7g_Rc10o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_incorrect = dev_data[(cond1) | (cond2)| (cond3)][show_columns]\n",
        "csv_correct = dev_data.drop(csv_incorrect.index)[show_columns]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_ADM5iYc10u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_incorrect.to_csv('csv_incorrect.csv')\n",
        "csv_correct.to_csv('csv_correct.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}